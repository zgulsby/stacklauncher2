# RunPod Stack Configuration Template
# This file defines your GPU-based deployment stack for the RunPod Infrastructure Catalog

# ===== REQUIRED FIELDS =====

# Basic metadata
id: "my-stack-id"           # Must be lowercase, alphanumeric with hyphens only
name: "My Stack Name"       # Human-readable name
version: "1.0.0"           # Semantic version (x.y.z)
description: "Description of what this stack does"
author: "Your Name or Organization"

# Container configuration
containerImage: "your-registry/your-image:tag"  # Docker image to deploy

# Resource requirements
gpu:
  type: "RTX 4090"         # GPU type: RTX 4090, A100, A6000, V100, etc.
  count: 1                 # Number of GPUs required
  # memoryGB: 24           # Optional: Specific GPU memory requirement

memory: 16                 # System memory in GB (required)

# Documentation (required)
docs:
  description: "Detailed description of your stack"
  usage: |
    Instructions for using this stack:
    1. Step one
    2. Step two
    3. Step three


# ===== OPTIONAL FIELDS =====

# Container overrides
# entrypoint: ["/bin/bash", "-c"]        # Override container entrypoint
# command: ["./start.sh"]                # Override container command
# workingDir: "/workspace"               # Set working directory

# Additional resources
# cpu: 4                                 # CPU cores (defaults to 1)

# Network configuration
# ports:
#   - containerPort: 8080                # Port inside container
#     exposedPort: 8080                  # Port to expose (optional, defaults to containerPort)
#     protocol: "tcp"                    # Protocol: tcp or udp (defaults to tcp)
#   - containerPort: 7860
#     exposedPort: 7860
#     protocol: "tcp"

# Environment variables
# env:
#   - name: "MODEL_NAME"
#     value: "default-model"
#     required: true                     # Mark as required for deployment
#   - name: "API_KEY"
#     value: ""
#     required: false                    # Optional variable

# Volume mounts
# volumes:
#   - containerPath: "/data"             # Mount point in container
#     size: 50                           # Size in GB
#     persistent: true                   # Keep data between deployments
#   - containerPath: "/models"
#     size: 100
#     persistent: false

# Extended documentation
# docs:
#   examples:                            # Usage examples
#     - "curl http://localhost:8080/health"
#     - "curl -X POST http://localhost:8080/predict -d '{\"input\": \"data\"}'"
#   troubleshooting: |                   # Common issues and solutions
#     Common issues:
#     - Issue 1: Solution 1
#     - Issue 2: Solution 2
#   changelog: |                         # Version history
#     v1.0.0: Initial release

# Security settings
# secure:
#   allowPrivileged: false               # Allow privileged container
#   readOnlyRootFilesystem: false        # Make root filesystem read-only
#   runAsUser: 1000                      # Run as specific user ID
#   capabilities:                        # Linux capabilities
#     add: ["NET_ADMIN"]                 # Add capabilities
#     drop: ["ALL"]                      # Drop capabilities

# Additional metadata
# tags:                                  # Tags for categorization
#   - "ai"
#   - "machine-learning"
#   - "pytorch"
#   - "tensorflow"

# category: "AI/ML"                      # Stack category
# license: "MIT"                         # License type
# homepage: "https://your-project.com"   # Project homepage
# repository: "https://github.com/user/repo"  # Source repository 